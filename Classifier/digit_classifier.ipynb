{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:50:42.915039Z",
     "start_time": "2021-03-19T13:50:40.146643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision.transforms as transforms \n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from digit_train_dataset import digit_train_dataset\n",
    "from digit_test_dataset import digit_test_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:51:53.715568Z",
     "start_time": "2021-03-19T13:51:53.712991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:51:56.185296Z",
     "start_time": "2021-03-19T13:51:56.144880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Load Data\n",
    "train_dataset = ImageFolder(root = './dataset_images_keras/train/',\n",
    "                             transform = train_transforms)\n",
    "test_dataset = ImageFolder(root = './dataset_images_keras/test/',\n",
    "                             transform = test_transforms)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:52:00.236057Z",
     "start_time": "2021-03-19T13:52:00.232933Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T21:11:39.274301Z",
     "start_time": "2021-03-18T21:11:39.148725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image(image, label, dataset):\n",
    "    print(f\"Label: {label}\") \n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    plt.show()\n",
    "\n",
    "show_image(*train_dataset[10], train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T20:51:07.118469Z",
     "start_time": "2021-03-18T20:51:06.927337Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogLeNet(\n",
       "  (conv1): BasicConv2d(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (conv2): BasicConv2d(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): BasicConv2d(\n",
       "    (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception3a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception3b): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception4a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception4b): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception4c): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception4d): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception4e): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception5a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception5b): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model = torchvision.models.googlenet(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T20:51:37.136563Z",
     "start_time": "2021-03-18T20:51:37.127573Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(1024, 500)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(500, 10)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model.fc = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T20:59:04.448622Z",
     "start_time": "2021-03-18T20:51:38.678087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 1.6454323786034433\n",
      "Cost at epoch 1 is 1.38206800695968\n",
      "Cost at epoch 2 is 1.3076217294283654\n",
      "Cost at epoch 3 is 1.252568888609812\n",
      "Cost at epoch 4 is 1.2014487021045597\n",
      "Cost at epoch 5 is 1.1876714030901592\n",
      "Cost at epoch 6 is 1.1860532466679403\n",
      "Cost at epoch 7 is 1.1521303250909396\n",
      "Cost at epoch 8 is 1.1313534768204712\n",
      "Cost at epoch 9 is 1.1213193500422995\n",
      "Cost at epoch 10 is 1.1187111386969752\n",
      "Cost at epoch 11 is 1.109891947001627\n",
      "Cost at epoch 12 is 1.0793025714077362\n",
      "Cost at epoch 13 is 1.0812305171195775\n",
      "Cost at epoch 14 is 1.079607795906938\n",
      "Cost at epoch 15 is 1.0471017994837128\n",
      "Cost at epoch 16 is 1.048043483740663\n",
      "Cost at epoch 17 is 1.0458481134889333\n",
      "Cost at epoch 18 is 1.0312264093525334\n",
      "Cost at epoch 19 is 1.0160739234049025\n",
      "Checking accuracy on Training Set\n",
      "Got 5438 / 7000 with accuracy 77.69\n",
      "Checking accuracy on Test Set\n",
      "Got 1057 / 1500 with accuracy 70.47\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.NLLLoss() \n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        scores = model.forward(data)\n",
    "\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Cost at epoch {epoch} is {sum(losses)/len(losses)}')\n",
    "\n",
    "# Check accuracy on training to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            scores = model.forward(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}') \n",
    "    \n",
    "    model.train()\n",
    "\n",
    "print(\"Checking accuracy on Training Set\")\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(test_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:52:18.257363Z",
     "start_time": "2021-03-19T13:52:17.945288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "documentation = {model: 0}\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:52:23.983725Z",
     "start_time": "2021-03-19T13:52:23.977804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(512, 500)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(500, 10)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model.fc = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T21:24:19.368865Z",
     "start_time": "2021-03-18T21:12:56.300741Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 1.2066661163007832\n",
      "Cost at epoch 1 is 0.8969151155861546\n",
      "Cost at epoch 2 is 0.8173648373449229\n",
      "Cost at epoch 3 is 0.7833944250187374\n",
      "Cost at epoch 4 is 0.7567558317151788\n",
      "Cost at epoch 5 is 0.7026795321675741\n",
      "Cost at epoch 6 is 0.6614474617455104\n",
      "Cost at epoch 7 is 0.6352756496978132\n",
      "Cost at epoch 8 is 0.64341360677595\n",
      "Cost at epoch 9 is 0.6243017825497884\n",
      "Cost at epoch 10 is 0.5890419100789719\n",
      "Cost at epoch 11 is 0.5523319651955338\n",
      "Cost at epoch 12 is 0.5459088352445054\n",
      "Cost at epoch 13 is 0.5359775814563716\n",
      "Cost at epoch 14 is 0.5076438250470924\n",
      "Cost at epoch 15 is 0.4975892592510676\n",
      "Cost at epoch 16 is 0.4783170244329052\n",
      "Cost at epoch 17 is 0.47889839286265307\n",
      "Cost at epoch 18 is 0.4736648327819833\n",
      "Cost at epoch 19 is 0.48888726457613246\n",
      "Checking accuracy on Training Set\n",
      "Got 6487 / 7000 with accuracy 92.67\n",
      "Checking accuracy on Test Set\n",
      "Got 1228 / 1500 with accuracy 81.87\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.NLLLoss() \n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        scores = model.forward(data)\n",
    "\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Cost at epoch {epoch} is {sum(losses)/len(losses)}')\n",
    "\n",
    "# Check accuracy on training to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            scores = model.forward(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}') \n",
    "    \n",
    "    model.train()\n",
    "\n",
    "print(\"Checking accuracy on Training Set\")\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(test_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:52:30.170135Z",
     "start_time": "2021-03-19T13:52:30.167832Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./Users/tobiasschulz/Documents/GitHub/digit_dataset/Classifier/saved_checkpoints/resnet/checkpoint.pth\"\n",
    "best_model_path = \"./Users/tobiasschulz/Documents/GitHub/digit_dataset/Classifier/saved_pytorch_models/resnet/checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:52:34.963861Z",
     "start_time": "2021-03-19T13:52:34.954756Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(start_epochs, n_epochs, valid_loss_min_input, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    Keyword arguments:\n",
    "    start_epochs -- the real part (default 0.0)\n",
    "    n_epochs -- the imaginary part (default 0.0)\n",
    "    valid_loss_min_input\n",
    "    loaders\n",
    "    model\n",
    "    optimizer\n",
    "    criterion\n",
    "    use_cuda\n",
    "    checkpoint_path\n",
    "    best_model_path\n",
    "    \n",
    "    returns trained model\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.NLLLoss() \n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = valid_loss_min_input \n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    for epoch in range(start_epochs, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # move to GPU\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model.forward(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, targets)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (x, y) in enumerate(test_loader):\n",
    "            # move to GPU\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            ## update the average validation loss\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model.forward(x)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, y)\n",
    "            # update average validation loss \n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "            _, predictions = output.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(test_loader.dataset)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "              \n",
    "        \n",
    "        # create checkpoint variable and add important data\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'valid_loss_min': valid_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "        # save checkpoint\n",
    "        save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "            # save checkpoint as best model\n",
    "            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "    \n",
    "    documentation = {model: float(num_correct)/float(num_samples)*100}\n",
    "    \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T13:53:01.716614Z",
     "start_time": "2021-03-19T13:52:37.231162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.000171 \tValidation Loss: 0.000644\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Users/tobiasschulz/Documents/GitHub/digit_dataset/Classifier/saved_checkpoints/resnet/checkpoint.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c5f4c3e7aa11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-bebeb7dcb50a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(start_epochs, n_epochs, valid_loss_min_input, checkpoint_path, best_model_path)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# save checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0msave_ckp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m## TODO: save the model if validation loss has decreased\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-230d40c3794c>\u001b[0m in \u001b[0;36msave_ckp\u001b[0;34m(state, is_best, checkpoint_path, best_model_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# save checkpoint data to the path given, checkpoint_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# if it is a best model, min validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cv_clone/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cv_clone/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cv_clone/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Users/tobiasschulz/Documents/GitHub/digit_dataset/Classifier/saved_checkpoints/resnet/checkpoint.pth'"
     ]
    }
   ],
   "source": [
    "trained_model = train(0, 20, 0.005, checkpoint_path, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documentation.json\", \"w\") as outfile: \n",
    "    json.dump(documentaion, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T21:40:16.073675Z",
     "start_time": "2021-03-18T21:40:16.071237Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"sample_file.json\", \"r+\") as file:\n",
    "    data = json.load(file)\n",
    "    data.update(documentation)\n",
    "    file.seek(0)\n",
    "    json.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
